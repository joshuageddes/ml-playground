{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "d_hidden = 2048\n",
    "h = 1\n",
    "d_k = d_v = d_model//h\n",
    "max_length = 256\n",
    "dict_size = 26\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.W1 = nn.Linear(d_model, d_hidden)\n",
    "        self.W2 = nn.Linear(d_hidden, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.W2(x)\n",
    "        return x\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedSelfAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        x = (Q@K.T) * (1/torch.sqrt(torch.tensor(d_k)))\n",
    "        mask = torch.triu(torch.ones_like(x, dtype=torch.bool),diagonal=1)\n",
    "        x = x.masked_fill(mask, float('-inf'))\n",
    "        return F.softmax(x, dim=1) @ V\n",
    "\n",
    "#single head for now. Will fix later\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.Q_proj = nn.Linear(d_model, d_k*h)\n",
    "        self.K_proj = nn.Linear(d_model, d_k*h)\n",
    "        self.V_proj = nn.Linear(d_model, d_v*h)\n",
    "        self.out_proj = nn.Linear(d_v*h, d_model)\n",
    "        self.self_attention = MaskedSelfAttention()\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        Q_inp = self.Q_proj(Q)\n",
    "        K_inp = self.K_proj(K)\n",
    "        V_inp = self.V_proj(V)\n",
    "        x = self.self_attention(Q_inp, K_inp, V_inp)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention()\n",
    "        self.ffn = FeedForward()\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.multi_head_attention(x, x, x) + x\n",
    "        x = self.attn_norm(x)\n",
    "        x = self.ffn(x) + x\n",
    "        x = self.ffn_norm(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.p_encode = torch.empty(max_length, d_model)\n",
    "        for k in range(max_length):\n",
    "            for i in range(d_model):\n",
    "                theta = torch.tensor(k/10000**(2*i/d_model))\n",
    "                if i%2 == 0:\n",
    "                    self.p_encode[k][i] = torch.sin(theta)\n",
    "                else:\n",
    "                    self.p_encode[k][i] = torch.cos(theta)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.p_encode[:x.shape[0]] + x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed = nn.Embedding(dict_size, d_model)\n",
    "        self.lm_head = nn.Linear(d_model, dict_size)\n",
    "        self.positional_encode = PositionalEncoding()\n",
    "        self.decoder_layers = [DecoderLayer() for _ in range(6)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.positional_encode(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        x = self.lm_head(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "input = torch.randint(0, 26, (10,))\n",
    "f = Transformer()\n",
    "out = f(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 26])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(26, 512)\n",
    "input = embed(torch.randint(0, 26, (10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1570, -0.0525,  1.9071, -0.0427],\n",
      "        [ 0.2758, -0.6716, -0.3214,  0.1559],\n",
      "        [-1.3432, -1.0895, -0.6403,  0.0365]])\n",
      "tensor([[-1.1966, -0.1959,  1.5795, -0.1871],\n",
      "        [ 1.0966, -1.4001, -0.4771,  0.7806],\n",
      "        [-1.1150, -0.6308,  0.2268,  1.5190]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn((3,4))\n",
    "print(tensor1)\n",
    "f = nn.LayerNorm(4)\n",
    "tensor1 = f(tensor1)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([4, 3, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 6\u001b[0m (\u001b[43mtensor1\u001b[49m\u001b[38;5;129;43m@tensor2\u001b[39;49m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn((3,4,5))\n",
    "tensor2 = torch.randn((3,4,5))\n",
    "tensor2 = tensor2.transpose(0,1)\n",
    "print(tensor1.shape)\n",
    "print(tensor2.shape)\n",
    "(tensor1@tensor2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]],dtype=torch.float32)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = torch.triu(torch.ones_like(matrix, dtype=torch.bool),diagonal=1)\n",
    "\n",
    "# Replace the upper triangle elements with negative infinity\n",
    "masked_matrix = matrix.masked_fill(mask, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
