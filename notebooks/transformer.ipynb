{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      4\u001b[0m d_k \u001b[38;5;241m=\u001b[39m d_v \u001b[38;5;241m=\u001b[39m d_model\u001b[38;5;241m/\u001b[39mh\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFeedForward\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28msuper\u001b[39m(FeedForward, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "d_hidden = 2048\n",
    "h = 1\n",
    "d_k = d_v = d_model/h\n",
    "max_length = 256\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.W1 = nn.Linear(d_model, d_hidden)\n",
    "        self.W2 = nn.Linear(d_hidden, d_model)\n",
    "        self.ffStack = nn.Sequential(\n",
    "            self.W1(),\n",
    "            nn.ReLU(),\n",
    "            self.W2()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ffStack(x)\n",
    "    \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        return F.softmax((Q@K.T) * (1/torch.sqrt(d_k)), dim=1) @ V\n",
    "\n",
    "#single head for now. Will fix later\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.Q_proj = nn.Linear(d_model, d_k*h)\n",
    "        self.K_proj = nn.Linear(d_model, d_k*h)\n",
    "        self.V_proj = nn.Linear(d_model, d_v*h)\n",
    "        self.out_proj = nn.Linear(d_v*h, d_model)\n",
    "        self.self_attention = SelfAttention()\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        Q_inp = self.Q_proj(Q)\n",
    "        K_inp = self.K_proj(K)\n",
    "        V_inp = self.V_proj(V)\n",
    "        attn = self.self_attention(Q_inp, K_inp, V_inp)\n",
    "        return self.out_proj(attn)\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention()\n",
    "        self.ffn = FeedForward()\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.multi_head_attention(x, x, x) + x\n",
    "        x = self.attn_norm(x)\n",
    "        x = self.ffn(x) + x\n",
    "        x = self.ffn_norm(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.p_encode = torch.empty(max_length, d_model)\n",
    "        for k in range(max_length):\n",
    "            for i in range(d_model):\n",
    "                theta = k/10000**(2*i/d_model)\n",
    "                if i%2 == 0:\n",
    "                    self.p_encode[k][i] = torch.sin(theta)\n",
    "                else:\n",
    "                    self.p_encode[k][i] = torch.cos(theta)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.p_encode + x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4547,  2.3426, -0.3314,  0.3432],\n",
      "        [-1.8603, -2.8984, -0.2083, -0.5121],\n",
      "        [-1.6965, -0.9645,  0.4901, -0.9244]])\n",
      "tensor([[ 0.4901,  1.3561, -1.2521, -0.5941],\n",
      "        [-0.4543, -1.4159,  1.0759,  0.7944],\n",
      "        [-1.1653, -0.2408,  1.5962, -0.1902]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn((3,4))\n",
    "print(tensor1)\n",
    "f = nn.LayerNorm(4)\n",
    "tensor1 = f(tensor1)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([4, 3, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 6\u001b[0m (\u001b[43mtensor1\u001b[49m\u001b[38;5;129;43m@tensor2\u001b[39;49m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn((3,4,5))\n",
    "tensor2 = torch.randn((3,4,5))\n",
    "tensor2 = tensor2.transpose(0,1)\n",
    "print(tensor1.shape)\n",
    "print(tensor2.shape)\n",
    "(tensor1@tensor2).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
